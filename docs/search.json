[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wendi’s Learning Diary",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "This is a book created from Wendi Li"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "WEEK1.html",
    "href": "WEEK1.html",
    "title": "Introduction",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n4+5\n\n[1] 9\n\n5+78\n\n[1] 83\n\n\n\n\n\n{#reference)"
  },
  {
    "objectID": "WEEK1.html#summary",
    "href": "WEEK1.html#summary",
    "title": "Getting start with remote sensing",
    "section": "Summary",
    "text": "Summary\nRemote sensing is the acquisition of information about an object or phenomenon without making physical contact with the object (from a distance), which is one of the specific methods of Earth observation. Remote sensing is an interesting and useful field with mass of data, it can detect the urban phenomena from a broader perspective and support for policy development. However, satellites also bring space junk. According to statistics, there are 3,000 dead satellites and around 34,000 pieces bigger than 10 centimetres in size littering space, which increases the hitting risk.\n\nSource: GisGeography\nRemote sensing is achieved through sensors, which can be on satellites or mounted on aircraft. There are two main categories of remote sensors, active sensors and passive sensors. Passive sensors receive and detect electromagnetic waves emitted by the target itself or reflected radiation from nature, with no emission source of its own. While active sensors emit electromagnetic waves to the target, which in turn receives their reflection. Each sensor has its own strengths and weaknesses (Figure 1), and using them in combination typically results in more accurate prediction data.\n\n\n\n\nTwo Types Of Sensors\n\n\n\nCreated by: Wendi Li source:EOS Data Analytics; GISGeography; Andrew’s Pages\n Raster data is the most common remote sensing data format, and its encoding methods mainly includes band interleaved by line (BIL), band sequential (BSQ), band interleaved by pixel (BIP) adn GeoTIFF (most common). To measure the quality and potential detail of imagery, four resolutions are used as metrics:\n\nSpatial Resolution  Size of the raster grid per pixel. Describing how detailed objects are in an image.\nSpectral Resolution Number and size of bands that a remote sensing platform can capture\n\nWavelength\nSpectral signature (discrete or continuous)\nConstrained to atmospheric windows\nMeasuring spectral reflectance - spectroradiometer\n\nTemporal Resolution Revisit time of sensor, which refers to the frequency at which imagery is recorded for a particular area.\nRadiometric Resolution The amount of information in each pixel, which refers to the ability of a sensor to identify and show small differences in energy\n\n\n\n\n\n\n\nSpatial Resolution\n\n\n\n\n\n\n\nSpectral Resolution\n\n\n\n\n\n\n\n\n\nTemporal Resolution\n\n\n\n\n\n\n\nRadiometric Resolution"
  },
  {
    "objectID": "WEEK1.html#application",
    "href": "WEEK1.html#application",
    "title": "Getting start with remote sensing",
    "section": "Application",
    "text": "Application"
  },
  {
    "objectID": "WEEK1.html#reflection",
    "href": "WEEK1.html#reflection",
    "title": "Getting start with remote sensing",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "WEEK3.html",
    "href": "WEEK3.html",
    "title": "4  Correction",
    "section": "",
    "text": "4.0.0.1 Geometric Correction\nGeometric correction in remote sensing is the digital process of matching the image projection to a specific projection surface. When remote sensing data is collected, the image will be distorted due to view angle, topography, wind, etc. To deal with the distortion, ground control points (GCP) plays a huge role. Ground control points are large marked targets on the ground, it can help remote sensing instruments to determine the exact geographical location of the image and then determin the geometric transformation coefficients. The linear algorithm is then used t o calculate the positions in the rectified (gold standard) map. Finally, using RMSE to test the error and choose the best model.\n\n\n\n\nflowchart LR\nA(Identifying Ground <br/> Control Points) --> B(Getting geometric <br/>transformation <br/> coefficients)\n    B --> C{Linear<br/> algorithm}\n    C -->|Forward| D[Using RMSE to <br/> test the error]\n    C -->|Backward| D[Using RMSE to <br/> test the error]\n\n\n\n\n\n\n\n\n\nCreated by: Wendi Li\n\n\n4.0.0.2 Orthorectification correction / Topographic correction\nWhen image distortion occurs by sensor orientation, topographical variation and the curvature of the earth, the Orthorectification process should be used to correct, which is always called topographic correction. To orthorectify an image, an elevation model and a rational polynomial coefficients (RPCs) are minimally required, which provide the information of the topography of the ground and the relationship between image and the ground. The accurate elevation model calculates the effect of terrain variation on the image pixels and determine the position, which corrects the distortion.\n\n\n\n\n\n\n\nDistortion\n\n\n\n\n\n\n\nElevation Model\n\n\n\n\n\n\nSource:Intermap\n\n\n4.0.0.3 Atmospheric Correction\nIn remote sensing detection, electromagnetic waves travel through atmosphere twice creating an adjacency effect, which is the reason for atmospheric correction. Atmospheric correction is mainly divided into relative correction and absolute correction, and they assume that a linear trend is existed for radiance between atmosphere and ground and the atmospheric measurements are available respectively. Dark Object Subtraction (DOS) is the most used methods to correct atmosphere and it is the most simplest one, and absolute correction methods are relatively difficult due to the unavailability of information they need. Empirical line correction is a special method which needs a reference spectrum from field or laboratory and use a linear regression to equate DN and reflectance.In my point of view, it is close to the relative correction method but cannot be fully attributed to it, because it is not exactly corrected by normalization.\n\n\n\n Created by: Wendi Li\n\n\n4.0.0.4 Radiometric Calibration"
  },
  {
    "objectID": "WEEK3.html#geometric-correction",
    "href": "WEEK3.html#geometric-correction",
    "title": "4  Correction",
    "section": "4.1 Geometric Correction",
    "text": "4.1 Geometric Correction\nGeometric correction in remote sensing is the digital process of matching the image projection to a specific projection surface. When remote sensing data is collected, the image will be distorted due to view angle, topography, wind, etc. To deal with the distortion, ground control points (GCP) plays a huge role. Ground control points are large marked targets on the ground, it can help remote sensing instruments to determine the exact geographical location of the image and then determin the geometric transformation coefficients. The linear algorithm is then used t o calculate the positions in the rectified (gold standard) map. Finally, using RMSE to test the error and choose the best model.\n\n\n\n\nflowchart LR\nA(Identifying Ground <br/> Control Points) --> B(Getting geometric <br/>transformation <br/> coefficients)\n    B --> C{Linear<br/> algorithm}\n    C -->|Forward| D[Using RMSE to <br/> test the error]\n    C -->|Backward| D[Using RMSE to <br/> test the error]\n\n\n\n\n\n\n\n\n\nCreated by: Wendi Li"
  },
  {
    "objectID": "WEEK3.html#orthorectification-correction-topographic-correction",
    "href": "WEEK3.html#orthorectification-correction-topographic-correction",
    "title": "4  Correction",
    "section": "4.2 Orthorectification correction / Topographic correction",
    "text": "4.2 Orthorectification correction / Topographic correction\nWhen image distortion occurs by sensor orientation, topographical variation and the curvature of the earth, the Orthorectification process should be used to correct, which is always called topographic correction. To orthorectify an image, an elevation model and a rational polynomial coefficients (RPCs) are minimally required, which provide the information of the topography of the ground and the relationship between image and the ground. The accurate elevation model calculates the effect of terrain variation on the image pixels and determine the position, which corrects the distortion.\n\n\n\n\n\n\n\nDistortion\n\n\n\n\n\n\n\nElevation Model\n\n\n\n\n\n\nSource:Intermap"
  },
  {
    "objectID": "WEEK3.html#atmospheric-correction",
    "href": "WEEK3.html#atmospheric-correction",
    "title": "4  Correction",
    "section": "4.3 Atmospheric Correction",
    "text": "4.3 Atmospheric Correction\nIn remote sensing detection, electromagnetic waves travel through atmosphere twice creating an adjacency effect, which is the reason for atmospheric correction. Atmospheric correction is mainly divided into relative correction and absolute correction, and they assume that a linear trend is existed for radiance between atmosphere and ground and the atmospheric measurements are available respectively. Dark Object Subtraction (DOS) is the most used methods to correct atmosphere and it is the most simplest one, and absolute correction methods are relatively difficult due to the unavailability of information they need. Empirical line correction is a special method which needs a reference spectrum from field or laboratory and use a linear regression to equate DN and reflectance.In my point of view, it is close to the relative correction method but cannot be fully attributed to it, because it is not exactly corrected by normalization.\n\n\n\n Created by: Wendi Li"
  },
  {
    "objectID": "WEEK3.html#radiometric-calibration",
    "href": "WEEK3.html#radiometric-calibration",
    "title": "4  Correction",
    "section": "4.4 Radiometric Calibration",
    "text": "4.4 Radiometric Calibration\nRadiometric Calibration is the ability to convert the uncalibrated and raw digital numbers (DN) for pixel value that recorded by satellite into physical units (W/m2/sr/µm), which aims to produce perfect images with regular coloration and correct exposure.\n\n\n\n\nFactors that affect radiance\n\n\n\nSource:Malvern Panalytical"
  },
  {
    "objectID": "WEEK3.html#summary",
    "href": "WEEK3.html#summary",
    "title": "Correction",
    "section": "Summary",
    "text": "Summary\n\n1. Correction\n\nGeometric Correction\nGeometric correction in remote sensing is the digital process of matching the image projection to a specific projection surface. When remote sensing data is collected, the image will be distorted due to view angle, topography, wind, etc. To deal with the distortion, ground control points (GCP) plays a huge role. Ground control points are large marked targets on the ground, it can help remote sensing instruments to determine the exact geographical location of the image and then determin the geometric transformation coefficients. The linear algorithm is then used t o calculate the positions in the rectified (gold standard) map. Finally, using RMSE to test the error and choose the best model.\n\n\n\n\nflowchart LR\nA(Identifying Ground <br/> Control Points) --> B(Getting geometric <br/>transformation <br/> coefficients)\n    B --> C{Linear<br/> algorithm}\n    C -->|Forward| D[Using RMSE to <br/> test the error]\n    C -->|Backward| D[Using RMSE to <br/> test the error]\n\n\n\n\n\n\n\n\n\nCreated by: Wendi Li\n\n\nOrthorectification correction / Topographic correction\nWhen image distortion occurs by sensor orientation, topographical variation and the curvature of the earth, the Orthorectification process should be used to correct, which is always called topographic correction. To orthorectify an image, an elevation model and a rational polynomial coefficients (RPCs) are minimally required, which provide the information of the topography of the ground and the relationship between image and the ground. The accurate elevation model calculates the effect of terrain variation on the image pixels and determine the position, which corrects the distortion.\n\n\n\n\n\n\n\nDistortion\n\n\n\n\n\n\n\nElevation Model\n\n\n\n\n\n\nSource:Intermap\n\n\nAtmospheric Correction\nIn remote sensing detection, electromagnetic waves travel through atmosphere twice creating an adjacency effect, which is the reason for atmospheric correction. Atmospheric correction is mainly divided into relative correction and absolute correction, and they assume that a linear trend is existed for radiance between atmosphere and ground and the atmospheric measurements are available respectively. Dark Object Subtraction (DOS) is the most used methods to correct atmosphere and it is the most simplest one, and absolute correction methods are relatively difficult due to the unavailability of information they need. Empirical line correction is a special method which needs a reference spectrum from field or laboratory and use a linear regression to equate DN and reflectance.In my point of view, it is close to the relative correction method but cannot be fully attributed to it, because it is not exactly corrected by normalization.\n\n\n\n Created by: Wendi Li\n\n\nRadiometric Calibration\nRadiometric Calibration is the ability to convert the uncalibrated and raw digital numbers (DN) for pixel value that recorded by satellite into physical units (W/m2/sr/µm), which aims to produce perfect images with regular coloration and correct exposure.\n\n\n\n\nFactors that affect radiance\n\n\n\nSource:Malvern Panalytical\n\n\n\n2. Merging and Enhancement\nRemote sensing data (e.g. Landsat, Sentinel) are obtained from constantly moving satellites at regular time intervals, which results in they consist of many separate tiles. If we want to get the data of a bigger area, imagery merging will be used, which is called as “Mosaicking” in remote sensing. When joining two or more tiles, it is better to choose the tile with close time and have less cloud so that the merging imagery will be clearer. After getting the image we need, some enhancement methods will be used to deal with the image to make it easier to analyze or bring out features, which including ratio, filtering, texture, data fusion and PCA.\n\n\n\nCreated by: Wendi Li"
  },
  {
    "objectID": "WEEK3.html#application",
    "href": "WEEK3.html#application",
    "title": "Correction",
    "section": "Application",
    "text": "Application\n\nApplication of correction: aerial photos\n\n\n\n\n\n\nPaper\n\n\n\nRocchini, D., Di Rita, A., 2005. Relief effects on aerial photos geometric correction. Applied Geography 25, 159–168. https://doi.org/10.1016/j.apgeog.2005.03.002s\n\n\nThis paper takes aerial photos as research object, applying geometric correction (called polynomial methods in the text) and orthorectification correction to aerial photos in three terrain types with different roughness (a flat area, a hilly area and a volcanic area) to test the effectiveness of them. This study was conducted based on Italy.\nFor geometric correction, 20 GCPs are selected randomly within the area and used in first and second order equation (the following figure). And the orthorectification correction uses an algorithm of the digital terrain model (DTM) which proposed by Konecny (1979) and Novak (1992). These two methods are used for three areas with different terrain roughness respectively, and then using the nearest neighbour method to output the images with 2 m pixel dimension, which compares the original image and corrected image. The RMSE for each area corresponding to each correction methods finally derived.\n\n\n\n\n(x,y): the coordinates of image to be rectified; (X,Y): coordinates of the reference image or map\n\n\n\nSource: Rocchini and Di Rita, 2005\nThe results show that these two methods have the similar performance in flat areas, but as the terrain gets steeper, orthorectification correction works better. And a worthwhile discussion point is the relationship between GCP and RMSE. In general, fewer GCPs can result in lower RMSE values, but a low RMSE value does not mean high accuracy. In fact, as the number of GCPs is increasing, the RMSE value will rise firstly then fall, and finally stabilize. And it is impossible to get a very low value of RMSE because it is sensitibve.\n\n\n\nSource: Rocchini and Di Rita, 2005\n\n\n\n\n\n\nMy comment\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\nApplication of image merging: the Cabo de Gata-Níjar Natural Park, Spain\n\n\n\n\n\n\nPaper\n\n\n\nRigol, J.P., Chica-Olmo, M., 1998. Merging remote-sensing images for geological-environmental mapping: application to the Cabo de Gata-Níjar Natural Park, Spain. Environmental Geology 34, 194–202. https://doi.org/10.1007/s002540050271\n\n\nThe Cabo de Gata Níjar Natural Park is a natural park in Spain which has numerous abandoned mineral resources. For this reason, many activities of mining and sulphide metals extraction has been occured throughout the region, which caused the pollution problem. Before the environmental study, researchers should identify the exact locations of the mine works and waste tips, which will use the image merging in remote sensing technology.\nSensors of SPOT HRV and Landsat Thematic Mapper (TM) are used in this study, which are commonly used in natural study. These two sensors have their own characteristics, so combination will provide researches more useful information. The merging process is mainly divide into two step. In the first step, a digital preprocessing is done to implement a physically coordinate fusing of them. In the second part, a merging of spatial and spectral information is implemented, which aims to combine the useful information of two data set into one. The basic principle of merging is that the original multispectral data is put into a new coordinate system through coordinate transform, where an axe represents the intensity image. The band of panchromatic data is then used to replace it, and finally the merging is completed by a reverse transform.\n\n\n\nSource: Rigol and Chica-Olmo, 1998\nThere are 5 methods of merging used in this study for testing, which includes IHS, SC, PCA, CN and HPF. The result shows that the HPF performs best with the least distortions, followed by PCA. THE IHS is the worst offender in this case. However, due to the lack of visual appeal of HPF images, the strong-versatility PCA has become the most widely used in current research.\n\n\n\nSource: Rigol and Chica-Olmo, 1998"
  },
  {
    "objectID": "WEEK3.html#personal-reflection",
    "href": "WEEK3.html#personal-reflection",
    "title": "Correction",
    "section": "Personal Reflection",
    "text": "Personal Reflection"
  },
  {
    "objectID": "WEEK6.html#summary",
    "href": "WEEK6.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\nRemote sensing images usually contain a lot of information, such as vegetation, sea, buildings, etc. These different classifications of information appear more complex in urban areas. When using remote sensing data for research, it is common to use some classified information instead of all, which is called classified data. The land cover is one of the typical and popular classified data, which is useful for urban planning and Disaster detection. For example, using EO data to detect urban green area achieve the physical accessibility, which improves the efficiency of government monitoring urban expansion and land use (Giuliani et al., 2021). And in the most cited paper on forest fire, different types of vegetation are detected from Landsat image to identify the flammable trees and obtain a fire hazard map(Chuvieco and Congalton, 1989).\n\n\n\n\n\n\n\nVegetated areas over the Geneva canton for 2019, computed with Sentinel-2 data\n\n\n\n\n\n\n\nVegetation map (different colors indicate different types of vegetation)\n\n\n\n\n\n\nSources: Giuliani et al., 2021; Chuvieco and Congalton, 1989\nBut how can we extract such information? This will involve machine learning, which is science of computer modeling of learning process.\nMachine learning (ML) is based on data and it can help us make decisions or predictions without explicit programming, which also known as training. There are many models in machine learning, including the linear regression model that many people are familiar with and some complex models such as tree-based model and neural network. In remote sensing area, tree-based model is mainly used to identiy. image. Tree-based model uses different input variables to make complex decisions, of which classification and regression trees (CART) and random forest models are most commonly used.\n\n\n\n\nOutline of Machine Learning\n\n\n\nCreated by: Wendi Li\nCART and random forest are both can predict large amounts of non-linear data, and neither requires pre-processing of dirty data (null values, outliers, etc.). The CART model includes classification tree and regression tree, which are used to classify data into several discrete categories and predict continuous dependent variable respectively. In classification tree, it uses Gini Impurity to split the data set to a decision tree, but in regression tree, the sum of squared residuals (SSR) is used. When decision tree is too deep and performing well in training data, overfitting occurs. Therefore, an optimized model called random forest is created to deal with more complex issues. Random forest consists of many decision trees, it uses bootstrapping to randomly draw N times from the data to get N samples (samples can be repeated) and determine the beat option.\n\n\n\n\n\n\n\nCART\n\n\n\n\n\n\n\nRandom Foreprobabilityst\n\n\n\n\n\n\nSources: Digital Vidya; WIDIMEDIA COMMONS\nSo far, a new question will be raised: how do we apply the machine learning to imagery classification in remote sensing? This will refer to the supervised and unsupervised model and two typical application in remote sensing area.\nUnsupervised classification does not have any prior information in the classification process and automatically classify data based on the distribution pattern of the spectral features. The methods of unsupervised classification includes K-means clustering, ISODATA algorithm etc., and DBSCAN in GIS is also a similar thing.\nSupervised classification is an identification method based on known categories in the specific area, which commonly has better accuracy than unsupervised. And this kind of classification always includes the same process of class definition, pre-processing, training, pixel assignment and accuracy assessment. Here, we focus on two popularly used methods, Maximum Likelihood Classification(MLC) and Support Vector Machines(SVM). Maximum Likelihood takes the image and assigns pixel to the most probable type, which is based on probability that we set. SVM is a linear binary classifiers that assign a given test sample a class from one of the two possible labels, which determines an optimal hyperplane to divide the data set. However, data from different categories are impossibly always separable by linear relationship. Therefore, a model based on kernel trick was proposed by Cortes and Vapnik in 1995, which improved the availability.\n\n\n\n\n\n\n\nBasic concepts of MLC\n\n\n\n\n\n\n\nTwo types of SVM\n\n\n\n\n\n\nSources: Sisodia et al., 2014;Sheykhmousa et al., 2020"
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Wendi’s Learning Diary",
    "section": "WHO AM I ?",
    "text": "WHO AM I ?\n\nI am Wendi Li\nA master student from Centre for Advanced Spatial Analysis (CASA), UCL. My undergraduate degree is from Guangdong University of Foreign Studies, majoring in logistics management. What motivated me to change my major and enter CASA is my keen interest in data science and urban studies. Now my research interests are focusing on data visualization, web gis and connected environment. I have a love and aspirations for academic research and desire for progress. I would appreciate your comments on my learning diary and welcome you to discuss with me!\nEmail: wendi_li1002@outlook.com / ucfnlih@ucl.ac.uk"
  },
  {
    "objectID": "WEEK2.html#slides",
    "href": "WEEK2.html#slides",
    "title": "A Small Presentation Of LiDAR",
    "section": "slides",
    "text": "slides"
  },
  {
    "objectID": "WEEK7.html#summary",
    "href": "WEEK7.html#summary",
    "title": "Classification II",
    "section": "Summary",
    "text": "Summary\nImage analysis is divided into pixel-based and object-based. Pixel-based analysis is the traditional classification method which focuses on separated pixel and spectral information but not detect the boundary. While Object-Based Image Analysis (OBIA) segments the image into some vector images that consist of several small pixels. Segmentation and classification are two principles of OBIA, it firstly segments the image into the meaningful physical objects and then classifies them into different categories according to spectral, geometrical, and spatial properties. Nowadays, the vast majority of scholars believe that OBIA has higher accuracy than pixel-based analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nSources:Crommelinck et al., 2016; Stack\nAfter finishing the implementation part of classification, the assessment/test part should be done to make sure the accuracy of the results. So first of all, we should know how to get the test data. A good approach is to split the whole data set into train part and test part, and most people think that the ratio of 80:20 is appropriate. After splitting, cross validation should be used to prevent overfitting and gives a more accurate estimate of the model’s performance. However, the original cross validation method is only applicable to non-spatial data, for remote sensing data, the spatial cross validation will be used.\n\n\n\n\nOriginal cross validation\n\n\n\nSource:scikit-learn\nIn spatial data, two close regions are likely to have a lot of similarities, which may cause data leakage. Using spatial cross validation method, the overfitting in spatial data could be solved. The basic principle of spatial cross validation is to split the data into folds or partitions so that each partition contains a representative sample of the spatial distribution of the data. Scikit-learn’s built-in CV functions provide the code to achieve this.\n\n\n\nSource: Lovelace, et al."
  },
  {
    "objectID": "WEEK7.html#application",
    "href": "WEEK7.html#application",
    "title": "Classification II",
    "section": "Application",
    "text": "Application\n\nApplication of image analysis method comparison: LULC\n\n\n\n\n\n\nPaper\n\n\n\nDuro, D.C., Franklin, S.E., Dubé, M.G., 2012. A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery. Remote Sensing of Environment 118, 259–272. https://doi.org/10.1016/j.rse.2011.11.020 [Link]\n\n\nThis paper selects the South Saskatchewan River as study area and the land use and land cover (LULC) as study object, applying three supervised machine learning algorithms both in pixel-based and object-based image analysis using SPOT-5 HRG imagery, to compare the performance of them. The three algorithms are decision tree (DT), random forest (RF), and the support vector machine (SVM).\nThe methodology includes data collection, image correction, image segmentation, object feature selection etc. Here, we focus on “Image segmentation and object feature selection”. Segmentation is the first step of object-based analysis. In this study, the multi-resolution segmentation (MRS) algorithm is used to segment image, which let pixel sized objects iteratively grown based on several user-defined parameters (scale, color/shape, smoothness/compactness) to define the size and shape of images. The selection of “scale” parameter is important for MRS algorithm because it will affect the accuracy of the output. The following figure shows the results of different values. After segmentation, classification process will be implemented, which all input layers, segmentation scales and object feature are used.\n\n\n\n\nComparison of image segmentation levels used in object-based classification: A) SPOT-5 10 m HRG false color image of study area (R—NIR, G—Red, B—Green); B) Image segmentation (MRS scale 5); C) Image segmentation (MRS scale 15); D) Image segmentation (MRS scale 30)\n\n\n\nSource: Duro et al., 2012\n\n\n\n\n\n\nMy comment on methodology\n\n\n\nHere we focus on the methodology of “Image segmentation and object feature selection”. In this paper, multi-resolution segmentation (MRS) algorithm is used to segmented. Due to the iterative ways, the results will be more accurate and robust. But one difficult point is that we should choose parameters for each image, this will be a challenge because the wrong selection may result in segmentation errors. And the high requirement of memory and computer will be another challenge when the resource is limited."
  },
  {
    "objectID": "WEEK7.html#personal-reflection",
    "href": "WEEK7.html#personal-reflection",
    "title": "Classification II",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week, we built on the previous week’s learning about classification."
  },
  {
    "objectID": "WEEK8.html#summary",
    "href": "WEEK8.html#summary",
    "title": "Temperature and Policy",
    "section": "Summary",
    "text": "Summary\nTemperature and weather can sometimes bring problems to cities. To which Urban Heat Island (UHI) is one of the most typical issues."
  },
  {
    "objectID": "WEEK8.html#slide-title",
    "href": "WEEK8.html#slide-title",
    "title": "Habits",
    "section": "Slide Title",
    "text": "Slide Title\n\n\nTurn off alarm\nGet out of bed\n\n\n\nGet in bed\nCount sheep"
  },
  {
    "objectID": "WEEK4.html#summary-policy-and-city",
    "href": "WEEK4.html#summary-policy-and-city",
    "title": "Policy Application",
    "section": "Summary: Policy and City",
    "text": "Summary: Policy and City\n\nPolicy: Zero Emission Tokyo Strategy\n Tokyo is a city located at the head of Tokyo Bay, which is the capital and the most populous city of Japan. Tokyo always has a hot and humid climate, but With the urban expansion and global warming, the impact of climate is becoming more and more serious and even threatening people’s lives. In 2018, temperatures in Tokyo firstly reached 40 ºC and there were 29 consecutive days with temperature over 30 ºC in 2019. And the natural disasters such as rainstorm and tornado are occurring frequently. To deal with such climate problems, “Zero Emission Tokyo Strategy” was published in December 2019.\n\n\n\n The Zero Emission Tokyo Strategy is a Climate Action Plan that aims to achieve net zero CO2 emissions by 2050, and it also incorporates sustainable resource management into policies that contribute to reducing emissions even outside the city. The strategy focuses on reducing CO2 emissions and avoiding climate impacts, which is divided into three main steps in 2019, 2030 and 2050 respectively. Totally 14 specific policies are proposed in this action, here, I mainly focus on two of them that related to remote sensing: \n\nPolicy 3: Expansion of Zero Emission Buildings\n\nKey targets for 2030:\n\nReduce greenhouse gas emissions by 30% compared to 2000\nReduce energy consumption by 38% compared to 2000\nIncrease renewable power usage to 30%\n\nVision for 2050:\n\nHave all buildings in Tokyo be zero-emission buildings that account for adaptation measures such as disaster prevention and heat countermeasures\n\n\nPolicy 8: Fluorocarbons\n\nKey targets for 2030:\n\nReduce hydrofluorocarbons (HFCs) emissions by 35% compared to FY 2014\n\nVision for 2050:\n\nRealize zero fluorocarbon emissions by expanding the use of non-fluorocarbon equipment and strictly controlling equipment to prevent leakage of fluorocarbons during use and disposal."
  },
  {
    "objectID": "W7.html#summary",
    "href": "W7.html#summary",
    "title": "Classification II",
    "section": "Summary",
    "text": "Summary\nImage analysis is divided into pixel-based and object-based. Pixel-based analysis is the traditional classification method which focuses on separated pixel and spectral information but not detect the boundary. While Object-Based Image Analysis (OBIA) segments the image into some vector images that consist of several small pixels. Segmentation and classification are two principles of OBIA, it firstly segments the image into the meaningful physical objects and then classifies them into different categories according to spectral, geometrical, and spatial properties. Nowadays, the vast majority of scholars believe that OBIA has higher accuracy than pixel-based analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nSources:Crommelinck et al., 2016; Stack\nAfter finishing the implementation part of classification, the assessment/test part should be done to make sure the accuracy of the results. So first of all, we should know how to get the test data. A good approach is to split the whole data set into train part and test part, and most people think that the ratio of 80:20 is appropriate. After splitting, cross validation should be used to prevent overfitting and gives a more accurate estimate of the model’s performance. However, the original cross validation method is only applicable to non-spatial data, for remote sensing data, the spatial cross validation will be used.\n\n\n\n\nOriginal cross validation\n\n\n\nSource:scikit-learn\nIn spatial data, two close regions are likely to have a lot of similarities, which may cause data leakage. Using spatial cross validation method, the overfitting in spatial data could be solved. The basic principle of spatial cross validation is to split the data into folds or partitions so that each partition contains a representative sample of the spatial distribution of the data. Scikit-learn’s built-in CV functions provide the code to achieve this.\n\n\n\nSource: Lovelace, et al.\nAfter deciding the dataset, accuracy assessment should be implemented. The basic principle of all accuracy assessment is to compare the estimated value with the actual value and to quantify the difference between them. In remote sensing area, the estimated value are the classes mapped for each pixel, and the acutal value is the actual ground information in the areas corresponding to each pixel. The first step of accuracy assessment is to create validation data that should be known already. And then put image data and validation data into confusion matrix.\n\n\n\n\nConfusion Matrix\n\n\n\nSource: OPEN LIBRARY\nSeveral metrics are used to measure accuracy, such as producer’s accuracy, user’s accuracy and overall accuracy that related to accuracy, and errors of commission and errors of omission that related to error. The Kappa Coefficient is a general metric generated from a statistical test. Kappa Coefficient ranges from -1 t0 1, which a value of 0 indicates that the classification is no better than a random classification, a negative number indicates the classification is significantly worse than random and positive value means better."
  },
  {
    "objectID": "W7.html#application",
    "href": "W7.html#application",
    "title": "Classification II",
    "section": "Application",
    "text": "Application\n\nApplication of image analysis method comparison: LULC\n\n\n\n\n\n\nPaper\n\n\n\nDuro, D.C., Franklin, S.E., Dubé, M.G., 2012. A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery. Remote Sensing of Environment 118, 259–272. https://doi.org/10.1016/j.rse.2011.11.020 [Link]\n\n\nThis paper selects the South Saskatchewan River as study area and the land use and land cover (LULC) as study object, applying three supervised machine learning algorithms both in pixel-based and object-based image analysis using SPOT-5 HRG imagery, to compare the performance of them. The three algorithms are decision tree (DT), random forest (RF), and the support vector machine (SVM).\nThe methodology includes data collection, image correction, image segmentation, object feature selection etc. Here, we focus on “Image segmentation and object feature selection”. Segmentation is the first step of object-based analysis. In this study, the multi-resolution segmentation (MRS) algorithm is used to segment image, which let pixel sized objects iteratively grown based on several user-defined parameters (scale, color/shape, smoothness/compactness) to define the size and shape of images. The selection of “scale” parameter is important for MRS algorithm because it will affect the accuracy of the output. The following figure shows the results of different values. After segmentation, classification process will be implemented, which all input layers, segmentation scales and object feature are used.\n\n\n\n\nComparison of image segmentation levels used in object-based classification: A) SPOT-5 10 m HRG false color image of study area (R—NIR, G—Red, B—Green); B) Image segmentation (MRS scale 5); C) Image segmentation (MRS scale 15); D) Image segmentation (MRS scale 30)\n\n\n\nSource: Duro et al., 2012\n\n\n\n\n\n\nMy comment on methodology\n\n\n\nHere we focus on the methodology of “Image segmentation and object feature selection”. In this paper, multi-resolution segmentation (MRS) algorithm is used to segmented. Due to the iterative ways, the results will be more accurate and robust. But one difficult point is that we should choose parameters for each image, this will be a challenge because the wrong selection may result in segmentation errors. And the high requirement of memory and computer will be another challenge when the resource is limited."
  },
  {
    "objectID": "W7.html#personal-reflection",
    "href": "W7.html#personal-reflection",
    "title": "Classification II",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week, we built on the previous week’s learning about classification. I have understood how to decide the training data and testing data, and how to do the accuracy assessment after classification. I think the most interesting part in this week is the spatial cross validation which combining mathematics with spatial analysis. And the accuracy assessment is probably the most difficult part for me because it involves many models and calculations, but I also consider it is an very important part because it is necessary to do some detailed accuracy analysis but not juet put the metrics. So I think I will spend more time learning this part."
  },
  {
    "objectID": "WEEK4.html#summary",
    "href": "WEEK4.html#summary",
    "title": "Policy Application",
    "section": "Summary",
    "text": "Summary\n\nPolicy: Zero Emission Tokyo Strategy\n Tokyo is a city located at the head of Tokyo Bay, which is the capital and the most populous city of Japan. Tokyo always has a hot and humid climate, but With the urban expansion and global warming, the impact of climate is becoming more and more serious and even threatening people’s lives. In 2018, temperatures in Tokyo firstly reached 40 ºC and there were 29 consecutive days with temperature over 30 ºC in 2019. And the natural disasters such as rainstorm and tornado are occurring frequently. To deal with such climate problems, “Zero Emission Tokyo Strategy” was published in December 2019.\n\n\n\nSource: Zero Emission Tokyo Strategy  The Zero Emission Tokyo Strategy is a Climate Action Plan that aims to achieve net zero CO2 emissions by 2050, and it also incorporates sustainable resource management into policies that contribute to reducing emissions even outside the city. The strategy focuses on reducing CO2 emissions and avoiding climate impacts, which is divided into three main steps in 2019, 2030 and 2050 respectively. Totally 14 specific policies are proposed in this action, here, I mainly focus on two of them that related to remote sensing: \n\nPolicy 3: Expansion of Zero Emission Buildings\n\nKey targets for 2030:\n\nReduce greenhouse gas emissions by 30% compared to 2000\nReduce energy consumption by 38% compared to 2000\nIncrease renewable power usage to 30%\n\nVision for 2050:\n\nHave all buildings in Tokyo be zero-emission buildings that account for adaptation measures such as disaster prevention and heat countermeasures\n\n\nPolicy 8: Fluorocarbons\n\nKey targets for 2030:\n\nReduce hydrofluorocarbons (HFCs) emissions by 35% compared to FY 2014\n\nVision for 2050:\n\nRealize zero fluorocarbon emissions by expanding the use of non-fluorocarbon equipment and strictly controlling equipment to prevent leakage of fluorocarbons during use and disposal."
  },
  {
    "objectID": "WEEK4.html#application",
    "href": "WEEK4.html#application",
    "title": "Policy Application",
    "section": "Application",
    "text": "Application\nRemotely sensed data can provide some aerial information that is difficult to obtain, for example, green space within a city and the CO2 emissions in the air. Therefore, it always be applied in policy making or city monitoring. In this case, data from sensors LiDAR and SCISAT can be applied in policy 3 and policy 8 in Zero Emission Tokyo Strategy. The specific application process are as following, which uses 5 questions to structure:\n\nApplication of Policy 3: Expansion of Zero Emission Buildings\n\n\n\nSource: Zero Emission Tokyo Strategy\n\nWhat is the target and action?\n\nTarget: Reducing the emission that buildings produce.\nAction: Promoting the installation of renewable energy equipment, such as solar panels.\n\nWhat should government know?\n\nWhether a building is suitable for installing solar energy equipment.\nThe return on solar investment.\n\nWhat remotely sensed data and approach can be used?\n\nTypical representative: ABSOLAR\nData: LiDAR data set (3D point clouds, building heights and shapes, solar radiation value etc.)\nApproach:Using LiDAR data to developing high resolution 3D model of an entire urban area, and then using computer vision technology to recognise roof structures, roof objects, and skylights. Combining these two outputs to identify optimal rooftop areas, including shading analysis. The conclusion about whether a building is the suitable one could be decided until this step. In the last step, big Data analytics and data links would be uesd to explore the rate of return.\n\n\n\n\nApplication of Policy 8: Fluorocarbons\n\n\n\n\nWhat is the target and actions?\n\nTarget: Reducing hydrofluorocarbons (HFCs) emissions.\nAction: Expanding the use of non-fluorocarbon equipment.\n\nWhat should government know?\n\nMonitoring the changes of drofluorocarbons (HFCs) emissions.\n\nWhat remotely sensed data and approach can be used?\n\nData: ACE data set\nApproach: The Atmospheric Chemistry Experiment (ACE) on SCISAT was launched into a high-inclination orbit which can bring them to many regions of Earth, then using ACE FTS (Atmospheric Chemistry Experiment Fourier Transform Spectrometer) to measure."
  },
  {
    "objectID": "WEEK4.html#personal-reflection",
    "href": "WEEK4.html#personal-reflection",
    "title": "Policy Application",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week I have learned about how remotely sensed data apply in city planning, policy decision, action monitoring etc. This is really new to me because I have never thought about such methods to detect the urban phenomenon. What attracts me most is the case of drought - The California Department of Water Resources (CADWR) used Sentinel moisture index to spot who is watering over the limitation. This example makes me feel the power of remote sensing, which can not only identify phenomena on a large scale but can also for precise house. The even more advanced point is that it can also identify properties hidden in the soil.\nWhat I have learned from this week could give me strong support for future research or work.The combination of remotely sensed data and policy provides a complete knowledge hierarchy for me, which gives me a new understanding of both urban and global policy proposals. These will be the basics for my research on urban study. But one important thing is the policy making is definitely based on local situation and it will be adjusting. So if I really want to be a researcher who is familiar with urban policy and remote sensing data, more policy and literature should be read, it will be a long road."
  },
  {
    "objectID": "W8.html",
    "href": "W8.html",
    "title": "Temperature and Policy",
    "section": "",
    "text": "Summary\nTemperature and weather sometime bring issues to cities, such as heat wave and tornadoes. In response, governments and organizations worldwide are actively proposing policies and beginning actions to address environmental issues. Here, we focus on Urban Heat Island (UHI) as an example to show how temperature issue affects cities and how cities react to it.\nUrban heat island effect is a phenomenon in which the temperature in urban centers is significantly higher than in the surrounding rural areas. This is because of dense urban roads and buildings that are more likely to absorb heat, and the lack of vegetation that cools the environment. Urban heat island could cause many problem in cities, in terms of both economy and life. In Europe, over 4% of summer mortality is due to high temperature. And some studies show that high green house gas will cause a higher percent GDP lost.\n\n\n\n\nThe Impact Of Heat\n\n\n\nSource: ISGlobal\nMany global, metropolitan, local policies are made to deal with the city problems, however, are they all truly effective and implemented? The answer is no. In fact, some of them are not well thought out:\nProblem 1: Too much information Sample: Beating the Heat: A Sustainable Cooling Handbook for Cities\nBeating the Heat is offers city planners an encyclopaedia of proven options to deal with urban heat, which includes 80 supporting case studies and totally 208 pages. As the first guide for new planner, the odds is they will not read and engage with all of them. So the better way is to use baseline assessment to classify and simplify.\n\n\n\n\nBeating the Heat: A Sustainable Cooling Handbook for Cities\n\n\n\nSource:UPSC\nProblem 2: No specific consideration Sample: Western Sydney: Turn Down the Heat Strategy and Action, 2018\nTo react the urban heat in Western Sydney, “Turn Down the Heat Strategy and Action” was published in 2018. An some in-depth assessments had already been taken place, including the future of urban heat in Western Sydney and the impact of high temperature on human, economy and buildings. However, it is also listed in the Beating the Heat guide, which means that it wasn’t really implemented until 2021. This leads one to question: is the policy plan really just a plan?\nIn my opinion, this is majorly due to the strategy does not provide specific steps and methods for implementation. For example, what is the workflow and timeline of the action? And who will be involved in this action and what are their respective responsibilities? If thees specific things do not mention in the plan, the action will lack of guiding and performability and finally vacant.\n\n\n\n\nBeating the Heat: A Sustainable Cooling Handbook for Cities\n\n\n\nSource: WSROC"
  },
  {
    "objectID": "W8.html#summary",
    "href": "W8.html#summary",
    "title": "Temperature and Policy",
    "section": "Summary",
    "text": "Summary\nTemperature and weather sometime bring issues to cities, such as heat wave and tornadoes. In response, governments and organizations worldwide are actively proposing policies and beginning actions to address environmental issues. Here, we focus on Urban Heat Island (UHI) as an example to show how temperature issue affects cities and how cities react to it.\nUrban heat island effect is a phenomenon in which the temperature in urban centers is significantly higher than in the surrounding rural areas. This is because of dense urban roads and buildings that are more likely to absorb heat, and the lack of vegetation that cools the environment. Urban heat island could cause many problem in cities, in terms of both economy and life. In Europe, over 4% of summer mortality is due to high temperature. And some studies show that high green house gas will cause a higher percent GDP lost.\n\n\n\n\nThe Impact Of Heat\n\n\n\nSource: ISGlobal\nMany global, metropolitan, local policies are made to deal with the city problems, however, are they all truly effective and implemented? The answer is no. In fact, some of them are not well thought out:\nProblem 1: Too much information Sample: Beating the Heat: A Sustainable Cooling Handbook for Cities\nBeating the Heat is offers city planners an encyclopaedia of proven options to deal with urban heat, which includes 80 supporting case studies and totally 208 pages. As the first guide for new planner, the odds is they will not read and engage with all of them. So the better way is to use baseline assessment to classify and simplify.\n\n\n\n\nBeating the Heat: A Sustainable Cooling Handbook for Cities\n\n\n\nSource:UPSC\nProblem 2: No specific consideration Sample: Western Sydney: Turn Down the Heat Strategy and Action, 2018\nTo react the urban heat in Western Sydney, “Turn Down the Heat Strategy and Action” was published in 2018. An some in-depth assessments had already been taken place, including the future of urban heat in Western Sydney and the impact of high temperature on human, economy and buildings. However, it is also listed in the Beating the Heat guide, which means that it wasn’t really implemented until 2021. This leads one to question: is the policy plan really just a plan?\nIn my opinion, this is majorly due to the strategy does not provide specific steps and methods for implementation. For example, what is the workflow and timeline of the action? And who will be involved in this action and what are their respective responsibilities? If thees specific things do not mention in the plan, the action will lack of guiding and performability and finally vacant.\n\n\n\n\nBeating the Heat: A Sustainable Cooling Handbook for Cities\n\n\n\nSource: WSROC"
  },
  {
    "objectID": "W8.html#application",
    "href": "W8.html#application",
    "title": "Temperature and Policy",
    "section": "Application",
    "text": "Application\n\nStrategy: Heat Actions in Phoenix, Arizona\nUrban heat islands and high temperatures affect the living quality and even lives of Phoenix’s residents. The New York Times reported that a “relentless and lethal blanket of heat” had killed 20 people in Phoenix in just a week. A climate justice reporter even commented that “America’s hottest city is nearly unlivable in summer”. In the group presentation, we discussed how to design a green bus stop project to react to the heat. Here, I will focus on the heat actions that phoenix government proposed and discuss.\nHeat action 1: Heat Relief Network “The Heat Relief Network (Phoenix) offers free water and indoor locations to cool off. Locations on the downloadable map are marked hydration stations and/or cooling refuge locations for anyone needing to get out of the heat.” This action is available from May 1 to September 30.\n\n\n\n\nHeat Relief Network\n\n\n\nSource: City of Phoenix\n\n\n\n\n\n\nDiscussion\n\n\n\nThis is a very useful measure that allows people suffering from the heat to quickly know the nearest free water and indoor place, minimizing the damage caused by the heat. But website does not mention about how to access to this map, and I think this would be a very noteworthy point because user-friendly for all ages should be ensured. What’s more, if this network could be available for the whole year would be better.\n\n\nHeat action 2: The Cool Pavement Pilot Program “The Cool Pavement Pilot Project launched in 2018 with test projects in selected portions of eight neighborhoods and one city park to receive cool pavement treatment as part of a pilot project. And new projects have been initiatiated in 2021 to test alternative coating materials.”\n\n\n\n\nCool Pavement\n\n\n\nSource: City of Phoenix\n\n\n\n\n\n\nDiscussion\n\n\n\nThe best point in this action is that it is honestly testing and improving the schemes rather than just planning. In September 2021, the Phoenix Street Transportation Department and Office of Sustainability discussed the results of the first year and affirmed its usefulness and operability. In October 2021, the pilot program ended and cool pavement becomes a regular program."
  },
  {
    "objectID": "W8.html#personal-reflection",
    "href": "W8.html#personal-reflection",
    "title": "Temperature and Policy",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week I have learned about how policies apply in urban issues, especially for temperature. There are many samples and cases provided in the slides, which deeply broadened my knowledge system. After reading such amount of policies and strategies, I have two thoughts:\nThe first one is, although there are many similarities in problems and solutions between cities, I found that all metropolitan policy reveal that they are very well adapted to themselves. For example, if a city is very dry, its cool action may more focus on planting trees rather than build cool equipments. This is why we need global policies for guidance, but also need the adapted local policies.\nThe second one is I realized that the official policies do not mean absolute correctness. We need to maintain critical and objective to think rationally about the strengths and weaknesses of each policy and strategy, and use expertise to discuss and enhance it, only then can urban problems be better solved."
  },
  {
    "objectID": "WEEK5.html#summary",
    "href": "WEEK5.html#summary",
    "title": "Google Earth Engine",
    "section": "Summary",
    "text": "Summary\nGoogle Earth Engine (GEE) is a platform for scientific analysis and visualization of geospatial data sets for academic, public interest, corporate and government users. It is an online software that stores data on servers, and the code is run on the client side - browser. Because of this characteristic, GEE can achieve geospatial analysis with massive data sets within seconds, which gives it a huge advantage. Some terms in GEE have been renamed, for example, the raster and vector data are called “Image” and “Feature” in GEE. And GEE uses the website programming language - Javascript to conduct.\n\n\n\nSource: Bikesh Bade\nNow, let’s turn to begin the GEE in action! The first thing we need to know what GEE code editor looks like and its function. In general, it has four parts: The bottom of the page is the map, which you can slide and mark randomly; the middle of the top is the code editor, the left side of it mainly displays the online project you created and the uploaded files, and the right side is the output area.\n\n\n\nSource: Andy MacLachlan\nAfter understanding the GEE code editor pages, turning to some methods and coding:\n\nLoading feature collection\nIf we want to know the geometries and features of an area, we can upload the shapefile and load it, which can output a feature collection (India). If we just want the polygon and single point rather than the whole area, we can filter it (Dheli).\n\n// Loading a feature collection\nvar India = ee.FeatureCollection('projects/ee-lwdfish/assets/gadm41_IND_2')\nprint(India, \"India\");\nMap.addLayer(India, {}, \"India\");\n\n//Loading just one polygon for point\nvar Dheli = ee.FeatureCollection('projects/ee-lwdfish/assets/gadm41_IND_2')\n    .filter('GID_1 == \"IND.25_1\"');\nprint(Dheli, \"Dheli\");\nMap.addLayer(Dheli, {}, \"Dheli\");\n\n\n\n\n\n\n\n\nIndia\n\n\n\n\n\n\n\nDheli\n\n\n\n\n\n\nSource: Output from practical, Wendi Li\n\n\nMosaic images\nWhen we look at images, it may be very apparent differences between the tiles (spatial mosaic), which is probably due to the date of collection and the atmospheric correction applied(Andy MacLachlan). To deal with this “mosaic images”, we can take the average value (mean).\n\n// Before averaging: Mosaic images\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\n// After averaging\nvar meanImage = oneimage_study_area_cloud_scale.mean();\n\nMap.addLayer(meanImage, vis_params2, 'mean');\n\n\n\n\n\n\n\n\nBefore averaging\n\n\n\n\n\n\n\nAfter averaging\n\n\n\n\n\n\nSource: Output from practical, Wendi Li\n\n\nTexture measures\nglcmTexture() function calculates the urface reflectance values and extracting statistical measures from this matrix to describe the texture. A important point here is we need to multiple the surface reflectance values to computer or the texture would just be 1s and 0s.\n\nvar glcm = clip.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'])\n  .multiply(1000)\n  .toUint16()\n  .glcmTexture({size: 1})\n  .select('SR_.._contrast|SR_.._diss')\n  .addBands(clip);\n  \n// add to the map, but change the range values  \nMap.addLayer(glcm, {min:14, max: 650}, 'glcm');\n\n\n\n\n\nTexture measures\n\n\n\nSource: Output from practical, Wendi Li"
  },
  {
    "objectID": "WEEK5.html#application",
    "href": "WEEK5.html#application",
    "title": "Google Earth Engine",
    "section": "Application",
    "text": "Application\n\nApplication of Google Earth Engine in wildfires model\n\n\n\n\n\n\nPaper\n\n\n\nSulova, A., Jokar Arsanjani, J., 2021. Exploratory Analysis of Driving Force of Wildfires in Australia: An Application of Machine Learning within Google Earth Engine. Remote Sensing 13, 10. https://doi.org/10.3390/rs13010010\n\n\n\n\n\n\nDefining the interest area of Australian mainland bounds based on GEE\n\n\n\nSource: Sulova and Jokar Arsanjani, 2021\n\n\n\n\n\n\nMy comment on methodology\n\n\n\nThis paper build a wildfire prediction model based on GEE platform. Here, the role of GEE is primarily to provide a large source of complete and free data, and provides a platform for the data-driven modeling. I think this is a typical and good sample for the usage of GEE, which it provides an idea about applying machine learning in GEE and it shows the data advantages that GEE has."
  },
  {
    "objectID": "WEEK5.html#personal-reflection",
    "href": "WEEK5.html#personal-reflection",
    "title": "Google Earth Engine",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week I have learned a very useful online platform - Google Earth Engine (GEE). I think the most helpful point of GEE is the convenience of obtaining data because we only need a small amount of code to use the online data, instead of going to different websites (e.g. Landsat, Sentinel) to download data and load it into the software again. In addition, the online operation makes it less storage or space requirements for our computers. Through reading papers, I also known that GEE platform can also be used to support model building.\nIn the later learning and research, I think GEE will be the most strong tool for me. But the biggest difficulty for me at the moment is that I have not used the Javascript language, resulting in I can’t understand some codes. Therefore, I will focus on learning the basic programming language in the next step of learning to support the subsequent use."
  },
  {
    "objectID": "WEEK6.html#application",
    "href": "WEEK6.html#application",
    "title": "Classification",
    "section": "Application",
    "text": "Application\n\nApplication of Supervised classification in village building\n\n\n\n\n\n\nPaper\n\n\n\nGuo, Z., Shao, X., Xu, Y., Miyazaki, H., Ohira, W., Shibasaki, R., 2016. Identification of Village Building via Google Earth Images and Supervised Machine Learning Methods. Remote Sensing 8, 271. https://doi.org/10.3390/rs8040271\n\n\nThis paper uses supervised machine learning to identify village buildings from open remote sensing data - Google Earth (GE) RGB images. Two specific algorithms are used for classification: adaptive boosting (AdaBoost) and convolutional neural networks (CNN). The purpose of this paper is to compare the performance of these two algorithms. And what I focus on here is the methodology part.\nFirst step for supervised classification is to customize the categories, researches define “village buildings” as any settlement with size less than 2 km here. The next step is to split data set into training data and test data. Four typical village/non-village areas are selected from the original image to be the training data (figure c,d,e,f), and each image is sized 600 × 900 pixels. The test data are contained within the entire area of figure a. After that, two algorithms are applied to the data, and results show that CNN better than AdaBoost.\n\n\n\n\nDataset for training and testing\n\n\n\nSource: Guo et al., 2016\n\n\n\n\n\n\nMy comment on methodology\n\n\n\nThis paper proposes a good research design and complete workflow to compare two ML algorithms based on village buildings. However, there is a serious issue in splitting training data and testing data. Researches select 4 areas from figure a as training data and the entire figure a as testing data, which causes data leakage because some data is duplicated. This method error may cause model instability and overfitting, and impact the results."
  },
  {
    "objectID": "WEEK6.html#personal-reflection",
    "href": "WEEK6.html#personal-reflection",
    "title": "Classification",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week I have learned about what is classification in remote sensing and how it is achieved by machine learning. I really liked the logic of this week’s lecture, it is progressive and clear:\n\n1. what is classification 2. The basic of machine learning 3. Machine learning apply in image classification\n\n And combining remote sensing, I think I am better understanding how machine learning work. I have learned ML in the previous course, but I still felt a little confused about its application and some concepts, especially the difference between supervised and unsupervised. But using the image classification, all things become clear because it is simple that supervised is assuming that we know the categories and unsupervised is not.\nThis let me feel the charm of interdisciplinary and knowledge integration, in which we may gain the different understanding of the same method. And the cross-thinking can provide more ideas for our research and benefit our critical thinking."
  }
]