[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wendi’s Learning Diary",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "This is a book created from Wendi Li"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "WEEK1.html",
    "href": "WEEK1.html",
    "title": "Introduction",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n4+5\n\n[1] 9\n\n5+78\n\n[1] 83\n\n\n\n\n\n{#reference)"
  },
  {
    "objectID": "WEEK1.html#summary",
    "href": "WEEK1.html#summary",
    "title": "Getting star with remote sensing",
    "section": "Summary",
    "text": "Summary\nRemote sensing is the acquisition of information about an object or phenomenon without making physical contact with the object (from a distance), which is one of the specific methods of Earth observation. Remote sensing is an interesting and useful field with mass of data, it can detect the urban phenomena from a broader perspective and support for policy development. However, satellites also bring space junk. According to statistics, there are 3,000 dead satellites and around 34,000 pieces bigger than 10 centimetres in size littering space, which increases the hitting risk.\n\nSource: GisGeography\nRemote sensing is achieved through sensors, which can be on satellites or mounted on aircraft. There are two main categories of remote sensors, active sensors and passive sensors. Passive sensors receive and detect electromagnetic waves emitted by the target itself or reflected radiation from nature, with no emission source of its own. While active sensors emit electromagnetic waves to the target, which in turn receives their reflection. Each sensor has its own strengths and weaknesses (Figure 1), and using them in combination typically results in more accurate prediction data.\n\n\n\n\nTwo Types Of Sensors\n\n\n\nCreated by: Wendi Li source:EOS Data Analytics; GISGeography; Andrew’s Pages\n Raster data is the most common remote sensing data format, and its encoding methods mainly includes band interleaved by line (BIL), band sequential (BSQ), band interleaved by pixel (BIP) adn GeoTIFF (most common). To measure the quality and potential detail of imagery, four resolutions are used as metrics:\n\nSpatial Resolution  Size of the raster grid per pixel. Describing how detailed objects are in an image.\nSpectral Resolution Number and size of bands that a remote sensing platform can capture\n\nWavelength\nSpectral signature (discrete or continuous)\nConstrained to atmospheric windows\nMeasuring spectral reflectance - spectroradiometer\n\nTemporal Resolution Revisit time of sensor, which refers to the frequency at which imagery is recorded for a particular area.\nRadiometric Resolution The amount of information in each pixel, which refers to the ability of a sensor to identify and show small differences in energy\n\n\n\n\n\n\n\nSpatial Resolution\n\n\n\n\n\n\n\nSpectral Resolution\n\n\n\n\n\n\n\n\n\nTemporal Resolution\n\n\n\n\n\n\n\nRadiometric Resolution"
  },
  {
    "objectID": "WEEK1.html#application",
    "href": "WEEK1.html#application",
    "title": "Getting star with remote sensing",
    "section": "Application",
    "text": "Application"
  },
  {
    "objectID": "WEEK1.html#reflection",
    "href": "WEEK1.html#reflection",
    "title": "Getting star with remote sensing",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "WEEK3.html",
    "href": "WEEK3.html",
    "title": "4  Correction",
    "section": "",
    "text": "4.0.0.1 Geometric Correction\nGeometric correction in remote sensing is the digital process of matching the image projection to a specific projection surface. When remote sensing data is collected, the image will be distorted due to view angle, topography, wind, etc. To deal with the distortion, ground control points (GCP) plays a huge role. Ground control points are large marked targets on the ground, it can help remote sensing instruments to determine the exact geographical location of the image and then determin the geometric transformation coefficients. The linear algorithm is then used t o calculate the positions in the rectified (gold standard) map. Finally, using RMSE to test the error and choose the best model.\n\n\n\n\nflowchart LR\nA(Identifying Ground <br/> Control Points) --> B(Getting geometric <br/>transformation <br/> coefficients)\n    B --> C{Linear<br/> algorithm}\n    C -->|Forward| D[Using RMSE to <br/> test the error]\n    C -->|Backward| D[Using RMSE to <br/> test the error]\n\n\n\n\n\n\n\n\n\nCreated by: Wendi Li\n\n\n4.0.0.2 Orthorectification correction / Topographic correction\nWhen image distortion occurs by sensor orientation, topographical variation and the curvature of the earth, the Orthorectification process should be used to correct, which is always called topographic correction. To orthorectify an image, an elevation model and a rational polynomial coefficients (RPCs) are minimally required, which provide the information of the topography of the ground and the relationship between image and the ground. The accurate elevation model calculates the effect of terrain variation on the image pixels and determine the position, which corrects the distortion.\n\n\n\n\n\n\n\nDistortion\n\n\n\n\n\n\n\nElevation Model\n\n\n\n\n\n\nSource:Intermap\n\n\n4.0.0.3 Atmospheric Correction\nIn remote sensing detection, electromagnetic waves travel through atmosphere twice creating an adjacency effect, which is the reason for atmospheric correction. Atmospheric correction is mainly divided into relative correction and absolute correction, and they assume that a linear trend is existed for radiance between atmosphere and ground and the atmospheric measurements are available respectively. Dark Object Subtraction (DOS) is the most used methods to correct atmosphere and it is the most simplest one, and absolute correction methods are relatively difficult due to the unavailability of information they need. Empirical line correction is a special method which needs a reference spectrum from field or laboratory and use a linear regression to equate DN and reflectance.In my point of view, it is close to the relative correction method but cannot be fully attributed to it, because it is not exactly corrected by normalization.\n\n\n\n Created by: Wendi Li\n\n\n4.0.0.4 Radiometric Calibration"
  },
  {
    "objectID": "WEEK3.html#geometric-correction",
    "href": "WEEK3.html#geometric-correction",
    "title": "4  Correction",
    "section": "4.1 Geometric Correction",
    "text": "4.1 Geometric Correction\nGeometric correction in remote sensing is the digital process of matching the image projection to a specific projection surface. When remote sensing data is collected, the image will be distorted due to view angle, topography, wind, etc. To deal with the distortion, ground control points (GCP) plays a huge role. Ground control points are large marked targets on the ground, it can help remote sensing instruments to determine the exact geographical location of the image and then determin the geometric transformation coefficients. The linear algorithm is then used t o calculate the positions in the rectified (gold standard) map. Finally, using RMSE to test the error and choose the best model.\n\n\n\n\nflowchart LR\nA(Identifying Ground <br/> Control Points) --> B(Getting geometric <br/>transformation <br/> coefficients)\n    B --> C{Linear<br/> algorithm}\n    C -->|Forward| D[Using RMSE to <br/> test the error]\n    C -->|Backward| D[Using RMSE to <br/> test the error]\n\n\n\n\n\n\n\n\n\nCreated by: Wendi Li"
  },
  {
    "objectID": "WEEK3.html#orthorectification-correction-topographic-correction",
    "href": "WEEK3.html#orthorectification-correction-topographic-correction",
    "title": "4  Correction",
    "section": "4.2 Orthorectification correction / Topographic correction",
    "text": "4.2 Orthorectification correction / Topographic correction\nWhen image distortion occurs by sensor orientation, topographical variation and the curvature of the earth, the Orthorectification process should be used to correct, which is always called topographic correction. To orthorectify an image, an elevation model and a rational polynomial coefficients (RPCs) are minimally required, which provide the information of the topography of the ground and the relationship between image and the ground. The accurate elevation model calculates the effect of terrain variation on the image pixels and determine the position, which corrects the distortion.\n\n\n\n\n\n\n\nDistortion\n\n\n\n\n\n\n\nElevation Model\n\n\n\n\n\n\nSource:Intermap"
  },
  {
    "objectID": "WEEK3.html#atmospheric-correction",
    "href": "WEEK3.html#atmospheric-correction",
    "title": "4  Correction",
    "section": "4.3 Atmospheric Correction",
    "text": "4.3 Atmospheric Correction\nIn remote sensing detection, electromagnetic waves travel through atmosphere twice creating an adjacency effect, which is the reason for atmospheric correction. Atmospheric correction is mainly divided into relative correction and absolute correction, and they assume that a linear trend is existed for radiance between atmosphere and ground and the atmospheric measurements are available respectively. Dark Object Subtraction (DOS) is the most used methods to correct atmosphere and it is the most simplest one, and absolute correction methods are relatively difficult due to the unavailability of information they need. Empirical line correction is a special method which needs a reference spectrum from field or laboratory and use a linear regression to equate DN and reflectance.In my point of view, it is close to the relative correction method but cannot be fully attributed to it, because it is not exactly corrected by normalization.\n\n\n\n Created by: Wendi Li"
  },
  {
    "objectID": "WEEK3.html#radiometric-calibration",
    "href": "WEEK3.html#radiometric-calibration",
    "title": "4  Correction",
    "section": "4.4 Radiometric Calibration",
    "text": "4.4 Radiometric Calibration\nRadiometric Calibration is the ability to convert the uncalibrated and raw digital numbers (DN) for pixel value that recorded by satellite into physical units (W/m2/sr/µm), which aims to produce perfect images with regular coloration and correct exposure.\n\n\n\n\nFactors that affect radiance\n\n\n\nSource:Malvern Panalytical"
  },
  {
    "objectID": "WEEK3.html#summary",
    "href": "WEEK3.html#summary",
    "title": "Correction",
    "section": "Summary",
    "text": "Summary\n\n1. Correction\n\nGeometric Correction\nGeometric correction in remote sensing is the digital process of matching the image projection to a specific projection surface. When remote sensing data is collected, the image will be distorted due to view angle, topography, wind, etc. To deal with the distortion, ground control points (GCP) plays a huge role. Ground control points are large marked targets on the ground, it can help remote sensing instruments to determine the exact geographical location of the image and then determin the geometric transformation coefficients. The linear algorithm is then used t o calculate the positions in the rectified (gold standard) map. Finally, using RMSE to test the error and choose the best model.\n\n\n\n\nflowchart LR\nA(Identifying Ground <br/> Control Points) --> B(Getting geometric <br/>transformation <br/> coefficients)\n    B --> C{Linear<br/> algorithm}\n    C -->|Forward| D[Using RMSE to <br/> test the error]\n    C -->|Backward| D[Using RMSE to <br/> test the error]\n\n\n\n\n\n\n\n\n\nCreated by: Wendi Li\n\n\nOrthorectification correction / Topographic correction\nWhen image distortion occurs by sensor orientation, topographical variation and the curvature of the earth, the Orthorectification process should be used to correct, which is always called topographic correction. To orthorectify an image, an elevation model and a rational polynomial coefficients (RPCs) are minimally required, which provide the information of the topography of the ground and the relationship between image and the ground. The accurate elevation model calculates the effect of terrain variation on the image pixels and determine the position, which corrects the distortion.\n\n\n\n\n\n\n\nDistortion\n\n\n\n\n\n\n\nElevation Model\n\n\n\n\n\n\nSource:Intermap\n\n\nAtmospheric Correction\nIn remote sensing detection, electromagnetic waves travel through atmosphere twice creating an adjacency effect, which is the reason for atmospheric correction. Atmospheric correction is mainly divided into relative correction and absolute correction, and they assume that a linear trend is existed for radiance between atmosphere and ground and the atmospheric measurements are available respectively. Dark Object Subtraction (DOS) is the most used methods to correct atmosphere and it is the most simplest one, and absolute correction methods are relatively difficult due to the unavailability of information they need. Empirical line correction is a special method which needs a reference spectrum from field or laboratory and use a linear regression to equate DN and reflectance.In my point of view, it is close to the relative correction method but cannot be fully attributed to it, because it is not exactly corrected by normalization.\n\n\n\n Created by: Wendi Li\n\n\nRadiometric Calibration\nRadiometric Calibration is the ability to convert the uncalibrated and raw digital numbers (DN) for pixel value that recorded by satellite into physical units (W/m2/sr/µm), which aims to produce perfect images with regular coloration and correct exposure.\n\n\n\n\nFactors that affect radiance\n\n\n\nSource:Malvern Panalytical\n\n\n\n2. Merging and Enhancement\nRemote sensing data (e.g. Landsat, Sentinel) are obtained from constantly moving satellites at regular time intervals, which results in they consist of many separate tiles. If we want to get the data of a bigger area, imagery merging will be used, which is called as “Mosaicking” in remote sensing. When joining two or more tiles, it is better to choose the tile with close time and have less cloud so that the merging imagery will be clearer. After getting the image we need, some enhancement methods will be used to deal with the image to make it easier to analyze or bring out features, which including ratio, filtering, texture, data fusion and PCA.\n\n\n\nCreated by: Wendi Li"
  },
  {
    "objectID": "WEEK3.html#application",
    "href": "WEEK3.html#application",
    "title": "Correction",
    "section": "Application",
    "text": "Application\n\nApplication of correction: aerial photos\n\n\n\n\n\n\nPaper\n\n\n\nRocchini, D., Di Rita, A., 2005. Relief effects on aerial photos geometric correction. Applied Geography 25, 159–168. https://doi.org/10.1016/j.apgeog.2005.03.002s\n\n\nThis paper takes aerial photos as research object, applying geometric correction (called polynomial methods in the text) and orthorectification correction to aerial photos in three terrain types with different roughness (a flat area, a hilly area and a volcanic area) to test the effectiveness of them. This study was conducted based on Italy.\nFor geometric correction, 20 GCPs are selected randomly within the area and used in first and second order equation (the following figure). And the orthorectification correction uses an algorithm of the digital terrain model (DTM) which proposed by Konecny (1979) and Novak (1992). These two methods are used for three areas with different terrain roughness respectively, and then using the nearest neighbour method to output the images with 2 m pixel dimension, which compares the original image and corrected image. The RMSE for each area corresponding to each correction methods finally derived.\n\n\n\n\n(x,y): the coordinates of image to be rectified; (X,Y): coordinates of the reference image or map\n\n\n\nSource: Rocchini and Di Rita, 2005\nThe results show that these two methods have the similar performance in flat areas, but as the terrain gets steeper, orthorectification correction works better. And a worthwhile discussion point is the relationship between GCP and RMSE. In general, fewer GCPs can result in lower RMSE values, but a low RMSE value does not mean high accuracy. In fact, as the number of GCPs is increasing, the RMSE value will rise firstly then fall, and finally stabilize. And it is impossible to get a very low value of RMSE because it is sensitibve.\n\n\n\nSource: Rocchini and Di Rita, 2005\n\n\n\n\n\n\nMy comment\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\nApplication of image merging: the Cabo de Gata-Níjar Natural Park, Spain\n\n\n\n\n\n\nPaper\n\n\n\nRigol, J.P., Chica-Olmo, M., 1998. Merging remote-sensing images for geological-environmental mapping: application to the Cabo de Gata-Níjar Natural Park, Spain. Environmental Geology 34, 194–202. https://doi.org/10.1007/s002540050271\n\n\nThe Cabo de Gata Níjar Natural Park is a natural park in Spain which has numerous abandoned mineral resources. For this reason, many activities of mining and sulphide metals extraction has been occured throughout the region, which caused the pollution problem. Before the environmental study, researchers should identify the exact locations of the mine works and waste tips, which will use the image merging in remote sensing technology.\nSensors of SPOT HRV and Landsat Thematic Mapper (TM) are used in this study, which are commonly used in natural study. These two sensors have their own characteristics, so combination will provide researches more useful information. The merging process is mainly divide into two step. In the first step, a digital preprocessing is done to implement a physically coordinate fusing of them. In the second part, a merging of spatial and spectral information is implemented, which aims to combine the useful information of two data set into one. The basic principle of merging is that the original multispectral data is put into a new coordinate system through coordinate transform, where an axe represents the intensity image. The band of panchromatic data is then used to replace it, and finally the merging is completed by a reverse transform.\n\n\n\nSource: Rigol and Chica-Olmo, 1998\nThere are 5 methods of merging used in this study for testing, which includes IHS, SC, PCA, CN and HPF. The result shows that the HPF performs best with the least distortions, followed by PCA. THE IHS is the worst offender in this case. However, due to the lack of visual appeal of HPF images, the strong-versatility PCA has become the most widely used in current research.\n\n\n\nSource: Rigol and Chica-Olmo, 1998"
  },
  {
    "objectID": "WEEK3.html#personal-reflection",
    "href": "WEEK3.html#personal-reflection",
    "title": "Correction",
    "section": "Personal Reflection",
    "text": "Personal Reflection"
  },
  {
    "objectID": "WEEK6.html#summary",
    "href": "WEEK6.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\nRemote sensing images usually contain a lot of information, such as vegetation, sea, buildings, etc. These different classifications of information appear more complex in urban areas. When using remote sensing data for research, it is common to use some classified information instead of all, which is called classified data. The land cover is one of the typical and popular classified data, which is useful for urban planning and Disaster detection. For example, using EO data to detect urban green area achieve the physical accessibility, which improves the efficiency of government monitoring urban expansion and land use (Giuliani et al., 2021). And in the most cited paper on forest fire, different types of vegetation are detected from Landsat image to identify the flammable trees and obtain a fire hazard map(Chuvieco and Congalton, 1989).\n\n\n\n\n\n\n\nVegetated areas over the Geneva canton for 2019, computed with Sentinel-2 data\n\n\n\n\n\n\n\nVegetation map (different colors indicate different types of vegetation)\n\n\n\n\n\n\nSources: Giuliani et al., 2021; Chuvieco and Congalton, 1989\nBut how can we extract such information? This will involve machine learning, which is science of computer modeling of learning process.\nMachine learning (ML) is based on data and it can help us make decisions or predictions without explicit programming, which also known as training. There are many models in machine learning, including the linear regression model that many people are familiar with and some complex models such as tree-based model and neural network. In remote sensing area, tree-based model is mainly used to identiy. image. Tree-based model uses different input variables to make complex decisions, of which classification and regression trees (CART) and random forest models are most commonly used.\n\n\n\n\nOutline of Machine Learning\n\n\n\nCreated by: Wendi Li\nCART and random forest are both can predict large amounts of non-linear data, and neither requires pre-processing of dirty data (null values, outliers, etc.). The CART model includes classification tree and regression tree, which are used to classify data into several discrete categories and predict continuous dependent variable respectively. In classification tree, it uses Gini Impurity to split the data set to a decision tree, but in regression tree, the sum of squared residuals (SSR) is used. When decision tree is too deep and performing well in training data, overfitting occurs. Therefore, an optimized model called random forest is created to deal with more complex issues. Random forest consists of many decision trees, it uses bootstrapping to randomly draw N times from the data to get N samples (samples can be repeated) and determine the beat option.\n\n\n\n\n\n\n\nCART\n\n\n\n\n\n\n\nRandom Foreprobabilityst\n\n\n\n\n\n\nSources: Digital Vidya; WIDIMEDIA COMMONS\nSo far, a new question will be raised: how do we apply the machine learning to imagery classification in remote sensing? This will refer to the supervised and unsupervised model and two typical application in remote sensing area.\nUnsupervised classification does not have any prior information in the classification process and automatically classify data based on the distribution pattern of the spectral features. The methods of unsupervised classification includes K-means clustering, ISODATA algorithm etc., and DBSCAN in GIS is also a similar thing.\nSupervised classification is an identification method based on known categories in the specific area, which commonly has better accuracy than unsupervised. And this kind of classification always includes the same process of class definition, pre-processing, training, pixel assignment and accuracy assessment. Here, we focus on two popularly used methods, Maximum Likelihood Classification(MLC) and Support Vector Machines(SVM). Maximum Likelihood takes the image and assigns pixel to the most probable type, which is based on probability that we set. SVM is a linear binary classifiers that assign a given test sample a class from one of the two possible labels, which determines an optimal hyperplane to divide the data set. However, data from different categories are impossibly always separable by linear relationship. Therefore, a model based on kernel trick was proposed by Cortes and Vapnik in 1995, which improved the availability.\n\n\n\n\n\n\n\nBasic concepts of MLC\n\n\n\n\n\n\n\nTwo types of SVM\n\n\n\n\n\n\nSources: Sisodia et al., 2014;Sheykhmousa et al., 2020"
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Wendi’s Learning Diary",
    "section": "WHO AM I ?",
    "text": "WHO AM I ?"
  },
  {
    "objectID": "WEEK2.html#slides",
    "href": "WEEK2.html#slides",
    "title": "A Small Presentation Of LiDAR",
    "section": "slides",
    "text": "slides"
  },
  {
    "objectID": "WEEK7.html#summary",
    "href": "WEEK7.html#summary",
    "title": "Classification II",
    "section": "Summary",
    "text": "Summary\nImage analysis is divided into pixel-based and object-based. Pixel-based analysis is the traditional classification method which focuses on separated pixel and spectral information but not detect the boundary. While Object-Based Image Analysis (OBIA) segments the image into some vector images that consist of several small pixels. Segmentation and classification are two principles of OBIA, it firstly segments the image into the meaningful physical objects and then classifies them into different categories according to spectral, geometrical, and spatial properties. Nowadays, the vast majority of scholars believe that OBIA has higher accuracy than pixel-based analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nSources:Crommelinck et al., 2016; Stack\nAfter finishing the implementation part of classification, the assessment/test part should be done to make sure the accuracy of the results. So first of all, we should know how to get the test data. A good approach is to split the whole data set into train part and test part, and most people think that the ratio of 80:20 is appropriate. After splitting, cross validation should be used to prevent overfitting and gives a more accurate estimate of the model’s performance. However, the original cross validation method is only applicable to non-spatial data, for remote sensing data, the spatial cross validation will be used.\n\n\n\n\nOriginal cross validation\n\n\n\nSource:scikit-learn\nIn spatial data, two close regions are likely to have a lot of similarities, which may cause data leakage. Using spatial cross validation method, the overfitting in spatial data could be solved. The basic principle of spatial cross validation is to split the data into folds or partitions so that each partition contains a representative sample of the spatial distribution of the data. Scikit-learn’s built-in CV functions provide the code to achieve this.\n\n\n\nSource: Lovelace, et al."
  },
  {
    "objectID": "WEEK7.html#application",
    "href": "WEEK7.html#application",
    "title": "Classification II",
    "section": "Application",
    "text": "Application\n\nApplication of image analysis method comparison: LULC\n\n\n\n\n\n\nPaper\n\n\n\nDuro, D.C., Franklin, S.E., Dubé, M.G., 2012. A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery. Remote Sensing of Environment 118, 259–272. https://doi.org/10.1016/j.rse.2011.11.020 [Link]\n\n\nThis paper selects the South Saskatchewan River as study area and the land use and land cover (LULC) as study object, applying three supervised machine learning algorithms both in pixel-based and object-based image analysis using SPOT-5 HRG imagery, to compare the performance of them. The three algorithms are decision tree (DT), random forest (RF), and the support vector machine (SVM).\nThe methodology includes data collection, image correction, image segmentation, object feature selection etc. Here, we focus on “Image segmentation and object feature selection”. Segmentation is the first step of object-based analysis. In this study, the multi-resolution segmentation (MRS) algorithm is used to segment image, which let pixel sized objects iteratively grown based on several user-defined parameters (scale, color/shape, smoothness/compactness) to define the size and shape of images. The selection of “scale” parameter is important for MRS algorithm because it will affect the accuracy of the output. The following figure shows the results of different values. After segmentation, classification process will be implemented, which all input layers, segmentation scales and object feature are used.\n\n\n\n\nComparison of image segmentation levels used in object-based classification: A) SPOT-5 10 m HRG false color image of study area (R—NIR, G—Red, B—Green); B) Image segmentation (MRS scale 5); C) Image segmentation (MRS scale 15); D) Image segmentation (MRS scale 30)\n\n\n\nSource: Duro et al., 2012\n\n\n\n\n\n\nMy comment on methodology\n\n\n\nHere we focus on the methodology of “Image segmentation and object feature selection”. In this paper, multi-resolution segmentation (MRS) algorithm is used to segmented. Due to the iterative ways, the results will be more accurate and robust. But one difficult point is that we should choose parameters for each image, this will be a challenge because the wrong selection may result in segmentation errors. And the high requirement of memory and computer will be another challenge when the resource is limited."
  },
  {
    "objectID": "WEEK7.html#personal-reflection",
    "href": "WEEK7.html#personal-reflection",
    "title": "Classification II",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis week, we built on the previous week’s learning about classification."
  }
]